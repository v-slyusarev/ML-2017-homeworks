{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретические задачи\n",
    "\n",
    "## 2.1 Bias-Variance-Noise decomposition\n",
    "Необходимо показать справедливость формулы:\n",
    "\n",
    "$\\mathbb{E} _{x, y} \\mathbb{E}_{X^l} (y - a_{X^l}(x))^2$ (полное матожидание ошибки)\n",
    "\n",
    "$= \\mathbb{E} _{x, y} (y - \\mathbb{E}(\\mathbf{y}|\\mathbf{x})^2$ (шум)\n",
    "\n",
    "$+ \\mathbb{E} _{x, y} (\\mathbb{E}(\\mathbf{y}|\\mathbf{x}) - \\mathbb{E}_{X^l} a_{X^l}(x))^2 $ (смещение)\n",
    "\n",
    "$ + \\mathbb{E} _{x, y} (a_{X^l}(x) - \\mathbb{E}_{X^l}a_{X^l}(x))^2$ (разброс)\n",
    "\n",
    "Распишем полное матожидание ошибки.\n",
    "\n",
    "$\\mathbb{E} _{x, y} \\mathbb{E}_{X^l} (y - a_{X^l}(x))^2 = \\text{<по телескопическому свойству условного матожидания>}\\ \\mathbb{E} _{x, y}\\mathbb{E}_{X^l} (\\mathbb{E} _{X^l, x, y} (y - a_{X^l}(x))^2\\ |\\ x))$\n",
    "\n",
    "Положим $y = f(x) + \\varepsilon$, $\\mathbb{E}\\varepsilon = 0, \\mathbb{D}\\varepsilon = \\sigma^2$\n",
    "\n",
    "$\\mathbb{E} _{X^l, x, y} ((y - a_{X^l}(x))^2\\ |\\ x) = \\mathbb{E} _{X^l, x, y} ((f(x) + \\varepsilon - a_{X^l}(x))^2\\ |\\ x) =  \\mathbb{E} _{X^l, x, y} ( \\varepsilon^2 +  (f(x) - a_{X^l}(x))^2 - 2 \\varepsilon(f(x) - a_{X^l}(x))\\ |\\ x) = $\n",
    "\n",
    "$= \\text{<в силу независимости шума и ошибки предсказания>} = $\n",
    "\n",
    "$ = \\mathbb{E} _{X^l, x, y} (\\varepsilon^2 \\ |\\ x) +  \\mathbb{E} _{X^l, x, y} ((f(x) - a_{X^l}(x))^2\\ |\\ x) - 2 \\mathbb{E} _{X^l, x, y} (\\varepsilon\\ |\\ x)\\mathbb{E} _{X^l, x, y} ((f(x) - a_{X^l}(x))\\ |\\ x) = $\n",
    "\n",
    "В силу независимости шума и признаков объекта, $\\mathbb{E} _{X^l, x, y} (\\varepsilon\\ |\\ x) = \\mathbb{E} _{X^l, x, y} (\\varepsilon) = \\mathbb{E} \\varepsilon = 0$, тогда последнее слагаемое зануляется. Первое, аналогично, равно $\\mathbb{E}\\varepsilon^2 = \\mathbb{D}\\varepsilon =  \\sigma^2$. Далее, распишем второе слагаемое, снова пользуясь телескопическим свойством УМО:\n",
    "\n",
    "$\\mathbb{E} _{X^l, x, y} ((a_{X^l} - f(x))^2\\ |\\ x) = \\mathbb{E} _{X^l, x, y} ( (a_{X^l}(x) - \\mathbb{E} _{X^l, x, y}a_{X^l}(x)) + \\mathbb{E} _{X^l, x, y}a_{X^l}(x) - f(x))^2\\ |\\ x)$\n",
    "\n",
    "Обозначим $\\bar{a}(x) := \\mathbb{E} _{X^l, x, y}a_{X^l}(x)$\n",
    "\n",
    "Тогда для второго слагаемого имеем \n",
    "\n",
    "$\\mathbb{E} _{X^l, x, y} ((f(x) - a_{X^l})^2\\ |\\ x) = \\mathbb{E} _{X^l, x, y} ( (a_{X^l}(x) - \\bar{a}(x) + \\bar{a}(x) - f(x))^2\\ |\\ x) = \\mathbb{E} _{X^l, x, y} ( (a_{X^l}(x) - \\bar{a}(x))^2 \\ |\\ x) + \\mathbb{E} _{X^l, x, y}((\\bar{a}(x) - f(x))^2\\ |\\ x) - 2 \\mathbb{E} _{X^l, x, y} ( a_{X^l}(x) - \\bar{a}(x) \\ |\\ x)\\mathbb{E} _{X^l, x, y}(\\bar{a}(x) - f(x)\\ |\\ x) =$\n",
    "\n",
    "$ = \\mathbb{E} _{X^l, x, y} ( (a_{X^l}(x) - \\bar{a}(x))^2 \\ |\\ x) + \\mathbb{E} _{X^l, x, y}((\\bar{a}(x) - f(x))^2\\ |\\ x) - 2 (\\bar{a}(x) - f(x)) \\mathbb{E} _{X^l, x, y} ( a_{X^l}(x) - \\bar{a}(x) \\ |\\ x)$\n",
    "\n",
    "В последнем члене заметим $2 (\\bar{a}(x) - f(x)) \\mathbb{E} _{X^l, x, y} ( a_{X^l}(x) - \\bar{a}(x) \\ |\\ x) = 2 (\\bar{a}(x) - f(x)) (\\mathbb{E} _{X^l, x, y}a_{X^l}(x) - \\mathbb{E} _{X^l, x, y}\\bar{a}(x))$ , что по определению $\\bar{a}(x)$ и телескопическому свойству равно $2 (\\bar{a}(x) - f(x)) (\\bar{a}(x) - \\bar{a}(x)) = 0$\n",
    "\n",
    "В итоге имеем $\\mathbb{E} _{X^l, x, y} ((f(x) - a_{X^l})^2\\ |\\ x) = \\mathbb{E} _{X^l, x, y} ( (a_{X^l}(x) - \\bar{a}(x))^2 \\ |\\ x) + \\mathbb{E} _{X^l, x, y}((\\bar{a}(x) - f(x))^2\\ |\\ x)$,\n",
    "\n",
    "и для полного матожидания, объединяя все выкладки, можно записать\n",
    "\n",
    "$\\mathbb{E} _{x, y} \\mathbb{E}_{X^l} (y - a_{X^l}(x))^2 = \\mathbb{E} _{x, y} ( (a_{X^l}(x) - \\bar{a}(x))^2 \\ |\\ x) + \\mathbb{E} _{x, y}((\\bar{a}(x) - f(x))^2\\ |\\ x) + \\sigma^2$,\n",
    "откуда прямо следует (сменой обозначений и снятием УМО по x) требуемое выражение.\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Смещение и разброс в бэггинге\n",
    "\n",
    "Пусть мы получили одинаково распределённые ответы $\\{a_m(x)\\}_{m=1}^M$ для $M$ алгоритмов, тогда ответ при бэггинге имеет вид $a(x) = \\frac{1}{M} \\sum_{m=1}^{M} a_m(x)$\n",
    "\n",
    "По определению распишем bias:\n",
    "\n",
    "$ \\mathbb{E} (a(x)) = \\mathbb{E} \\frac{1}{M} \\sum_{m=1}^{M} a_m(x) = \\frac{1}{M} \\sum_{m=1}^{M} \\mathbb{E} a_m(x) =  \\frac{1}{M} M \\cdot \\mathbb{E} a_1(x) = \\mathbb{E} a_1(x)$\n",
    "\n",
    "Видим, что bias не изменился. Посмотрим, что происходит с variance.\n",
    "\n",
    "$ \\mathbb{D} (a(x)^2) =  \\mathbb{D}(\\frac{1}{M}\\sum_{m=1}^{M} a_m(x))^2 = \\frac{1}{M^2}\\mathbb{D}\\sum_{i,j} a_i(x)a_j(x) = \\frac{1}{M}\\mathbb{D}a_1(x) + \\frac{1}{M^2}\\sum_{i \\ne j}cov(a_i(x), a_j(x))$\n",
    "\n",
    "Если ответы не коррелируют, второе слагаемое нулевое и variance уменьшается в m раз.\n",
    "## 2.3 Корреляция ответов базовых алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
